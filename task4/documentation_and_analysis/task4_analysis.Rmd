---
title: "R Notebook"
output: html_notebook
---

# Cleaning

Oh, boy.

## First steps

The three sheets are all organised and labelled differently, on top of having different questions, so the first thing I need to do is find commonalities to unify across.

I figured a good place to start was to import the three sheets, run `clean_names()` from `janitor` across them, and then see if there was any immediate common factors. From glancing at the tables I suspected that the header reformat for 2017 would mean it was entirely unique compared to the previous years.

I did this using `intersect()` in a form similar to this:
```{r eval=FALSE}
intersect(intersect(colnames(2015data),colnames(2016data)),colnames(2017data))
```
Unsurprisingly, I got nothing back. Intersecting just 2015 and 2016 returned 93 columns though, which is encouraging!

At this point, it makes the most sense to me to work on binding 2015 and 2016, then bring the combined might to bear on 2017.

## Dealing with 2015 and 2016

### Identifying differences

One final step was to use `setdiff()` to see specifically what columns 2015 and 2016 differed on:
```{r eval=FALSE}
setdiff(colnames(2015data), colnames(2016data))
setdiff(colnames(2016data), colnames(2015data))
```

This gives me two pieces of information immediately. 

PREFACE: this doesn't account for columns having the same data with a variant header -

* 2015 has 31 columns that 2016 doesn't
* 2016 has 30 columns that 2015 doesn't

I could also infer some things from the difference moving to 2016. We collected:

* Respondent metadata: gender, geographical
* More candy and treat questions
* More nonsense questions

The difference between the two more generally was a variance in what "treats" were included, and what the nonsense questions were.

### Bind the data

Since I had so many common columns, the first thing I did was use `bind_rows()` to combine the tables. I ended up with a table with an extra 30 columns in it compared to 2016, accounting for every column of variance identified earlier.

Next up, I was seeing quite a lot of NAs in some of the nonsense columns, so I checked if any were completely empty and discarded them from the table if they were.

```{r eval=FALSE}
bound_table <- bound_table %>% 
  remove_empty(which = "cols")
```

Turns out **10 columns were completely empty**.

### Organise this mess

The data broadly falls into three categories: 

* metadata
* [JOY/MEH/DESPAIR] treat ratings 
* freeform nonsense

As such it would be useful to arrange it that way before doing anything further.

In the context of this table, I'm considering metadata to be 

* Age
* Gender
* Country
* State/Province/County

Organising the treat rating columns, I discover that there are numerous columns that are meant to match up but have different names, so I return and amend those before the bind happens. For the sake of the basic analysis, I also choose to omit the entire freeform section from the tables. I might restore them later for fun stuff. Maybe.

After that is done, repeating the process again with 2017 is reasonably straightforward. I just need to start by trimming the Q#_ prefix from the requisite columns and renaming whatever is required to match previous convention.

### A note on duplicate columns

There's a few instances where the wording of the treat changes slightly between years, such as "Sweetums" becoming "Sweetums (a friend to diabetes)" and licorice changing to Not Black and Yes Black.

The reality is these could provoke the respondents into different answers, but for sake of simplicity I am combining where it seems reasonable.

# Analysis


```{r}
library(tidyverse)
library(here)
```

```{r}
source(here("data_cleaning_scripts/clean_data.R"))
halloween <- read_csv(here("clean_data/halloween_simplified.csv"))
```
```{r}
halloween %>% 
  summarise(across(6:last_col(), ~sum(!is.na(.)))) %>% 
  mutate(total = rowSums(.)) %>% 
  select(total)
```




```{r}
halloween %>% 
  filter(!is.na(age), going_out == "Yes") %>% 
  summarise(avg_age = round(mean(age),1))
```

```{r}
halloween %>% 
  filter(!is.na(age), going_out == "No") %>% 
  summarise(avg_age = round(mean(age), 1))
```

```{r}
halloween %>% 
  select(7:last_col()) %>% 
  pivot_longer(cols = everything(), names_to = "candy", values_to = "rating") %>% 
  group_by(candy, rating) %>% 
  filter(!is.na(rating)) %>% 
  summarise(count = n()) %>% 
  group_by(rating) %>% 
  slice_max(count)
```

```{r}
halloween %>% 
  group_by(starburst) %>% 
  filter(starburst == "DESPAIR") %>% 
  summarise(votes = n())
```

```{r}
halloween %>% 
  select(gender, 7:last_col()) %>% 
  pivot_longer(cols = 2:last_col(), names_to = "candy", values_to = "rating") %>% 
  group_by(candy, rating, gender) %>% 
  filter(!is.na(rating)) %>% 
  mutate(score = recode(rating, "DESPAIR" = -1, "JOY" = 1, "MEH" = 0)) %>% 
  summarise(score = sum(score)) %>% 
  group_by(gender, candy) %>% 
  summarise(score = sum(score)) %>% 
  group_by(gender) %>% 
  slice_max(score) %>% 
  arrange(-score)
```